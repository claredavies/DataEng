{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtiBJp7h6RwV"
      },
      "source": [
        "Team 3 - Web Testing\n",
        "\n",
        "Adapted from: \n",
        "\n",
        "https://github.com/nshepperd/gpt-2 \n",
        "\n",
        "https://github.com/minimaxir/gpt-2-simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8euSg4vt10ke",
        "outputId": "09a667f1-2b9a-4eaf-a574-6f765d544967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: toposort in /usr/local/lib/python3.7/dist-packages (1.7)\n"
          ]
        }
      ],
      "source": [
        "pip install toposort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_-68o8I18Qq",
        "outputId": "76aee249-a08e-4a1b-955d-5f25af127aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8GCG8FoR19tp"
      },
      "outputs": [],
      "source": [
        "import sys,os,json,shutil,re,urllib.request,time\n",
        "from tensorflow.python.client import device_lib\n",
        "from src.accumulate import AccumulatingOptimizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sys import exit\n",
        "from tensorflow.core.protobuf import rewriter_config_pb2\n",
        "from src import model, sample, encoder\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "from src.load_dataset import load_dataset, Sampler\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "#Feature selection\n",
        "#Restler output file may contain duplicate requests.\n",
        "#This method eliminates duplicate ones and also extract the desired features. We just need request_type , request_uri and request body\n",
        "def process_restler_output(restler_raw_file='test_cases_produced.csv',restler_processed_file=\"RESTler_unique_output.txt\"):\n",
        "  import csv, itertools\n",
        "  file = open(restler_raw_file)\n",
        "  csvreader = csv.reader(file)\n",
        "  header = next(csvreader)\n",
        "  print(header)\n",
        "  rows = []\n",
        "  for row in csvreader:\n",
        "      rows.append(row)\n",
        "  file.close()\n",
        "\n",
        "  for row in rows:\n",
        "      del row[5]\n",
        "      del row[4]\n",
        "      del row[0]\n",
        "\n",
        "  rows=[list(tupl) for tupl in {tuple(item) for item in rows }]\n",
        "  textfile = open(restler_processed_file, \"w\")\n",
        "  firstLine=True\n",
        "  for row in rows:\n",
        "      if row[2]:\n",
        "        element=\"HTTP \"+row[0]+\" \"+row[1]+\" \"+row[2]\n",
        "      else:\n",
        "        element=\"HTTP \"+row[0]+\" \"+row[1]\n",
        "      if(firstLine):\n",
        "        textfile.write(element)\n",
        "        firstLine=False\n",
        "      else: textfile.write( \"\\n\"+element)\n",
        "  textfile.close()\n",
        "  return restler_processed_file\n",
        "\n",
        "#It is needed to download the GPT-2 pre-trained model. there are four types: \n",
        "# 1. 124M (default)   2. 355M   3. 774M   4. 1558M\n",
        "#The last two models are large and cannot finetuned in the google Colab \n",
        "def get_model(model_type='124M'):\n",
        "\n",
        "\n",
        "    # create the directory if not exist\n",
        "    path = os.path.join('models', model_type)\n",
        "    if not os.path.exists(path):\n",
        "      os.makedirs(path)\n",
        "      url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\"+ model_type + \"/\" + \"checkpoint\"\n",
        "      urllib.request.urlretrieve(url, os.path.join(path, \"checkpoint\"))\n",
        "      url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\"+ model_type + \"/\" + \"encoder.json\"\n",
        "      urllib.request.urlretrieve(url, os.path.join(path, \"encoder.json\"))\n",
        "      url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\"+ model_type + \"/\" + \"hparams.json\"\n",
        "      urllib.request.urlretrieve(url, os.path.join(path, \"hparams.json\"))\n",
        "      url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\"+ model_type + \"/\" + \"model.ckpt.data-00000-of-00001\"\n",
        "      urllib.request.urlretrieve(url, os.path.join(path, \"model.ckpt.data-00000-of-00001\"))\n",
        "      url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\"+ model_type + \"/\" + \"model.ckpt.index\"\n",
        "      urllib.request.urlretrieve(url, os.path.join(path, \"model.ckpt.index\"))\n",
        "      url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\"+ model_type + \"/\" + \"model.ckpt.meta\"\n",
        "      urllib.request.urlretrieve(url, os.path.join(path, \"model.ckpt.meta\"))\n",
        "      url = \"https://openaipublic.blob.core.windows.net/gpt-2/models/\"+ model_type + \"/\" + \"vocab.bpe\"\n",
        "      urllib.request.urlretrieve(url, os.path.join(path, \"vocab.bpe\"))\n",
        "\n",
        "\n",
        "# return Tensorflow session. A Session places the graph ops onto Devices, such as CPUs or GPUs, and provides methods to execute them.\n",
        "def tensorflow_session():\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
        "    return tf.compat.v1.Session(config=config)\n",
        "\n",
        "#with fine tuning we can train the model on our specific dataset\n",
        "def fine_tuning(session,\n",
        "             restler_raw_file='test_cases_produced-4.csv',\n",
        "             steps=-1,\n",
        "             model_type='124M',\n",
        "             batch_size=1,\n",
        "             learning_rate=0.0001,\n",
        "             accumulate_gradients=5,\n",
        "             run_name='run1',\n",
        "             sample_length=1023,\n",
        "             only_train_transformer_layers=False,\n",
        "             optimizer='adam'):\n",
        "  \n",
        "    dataset=process_restler_output(restler_raw_file)\n",
        "\n",
        "\n",
        "    checkpoint_path = os.path.join('checkpoint', run_name)\n",
        "\n",
        "    try:\n",
        "      os.makedirs(checkpoint_path)\n",
        "    except:\n",
        "      pass\n",
        "    files = [f for f in os.listdir(checkpoint_path)]\n",
        "    for file in ['hparams.json', 'encoder.json', 'vocab.bpe']:\n",
        "        try:\n",
        "            shutil.copyfile(os.path.join('models', model_type, file),\n",
        "                            os.path.join(checkpoint_path, file))\n",
        "        except FileNotFoundError as fnf_error:\n",
        "            print(\"You need to download the GPT-2 model first via get_model()\")\n",
        "            raise(fnf_error)\n",
        "\n",
        "    enc = encoder.get_encoder(checkpoint_path)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if sample_length > hparams.n_ctx:\n",
        "        raise ValueError(\n",
        "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
        "    gpus = []\n",
        "\n",
        "    output = model.model(hparams=hparams, X=context, gpus=gpus, reuse=False)\n",
        "    loss = tf.reduce_mean(\n",
        "        input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
        "  \n",
        "\n",
        "    all_vars = [v for v in tf.compat.v1.trainable_variables() if 'model' in v.name]\n",
        "    #For models larger than 124M, it is better to set only_train_transformer_layers= true\n",
        "    train_vars = [v for v in all_vars if '/h' in v.name] if only_train_transformer_layers else all_vars\n",
        "\n",
        "    if optimizer == 'adam':\n",
        "        opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        " \n",
        "  # for models larger than 124M, it is better to set accumulate_gradients = 1\n",
        "  # accumulate_gradients = 1 means no accumulated grads\n",
        "    if accumulate_gradients > 1:\n",
        "\n",
        "        #It calculates the loss and gradients after each mini-batch, but instead of updating the model parameters, it waits and accumulates the gradients over consecutive batches.\n",
        "        opt = AccumulatingOptimizer(\n",
        "            opt=opt,\n",
        "            var_list=train_vars)\n",
        "        opt_reset = opt.reset()\n",
        "        opt_compute = opt.compute_gradients(loss)\n",
        "        opt_apply = opt.apply_gradients()\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', opt_apply)\n",
        "    else:\n",
        "        opt_grads = tf.gradients(ys=loss, xs=train_vars)\n",
        "        opt_grads = list(zip(opt_grads, train_vars))\n",
        "        opt_apply = opt.apply_gradients(opt_grads)\n",
        "        summary_loss = tf.compat.v1.summary.scalar('loss', loss)\n",
        "\n",
        "    summary_log = tf.compat.v1.summary.FileWriter(checkpoint_path)\n",
        "\n",
        "    saver = tf.compat.v1.train.Saver(\n",
        "        var_list=all_vars,\n",
        "        max_to_keep=1)\n",
        "    #execute the operation in the tensors\n",
        "    session.run(tf.compat.v1.global_variables_initializer())\n",
        "    ckpt = tf.train.latest_checkpoint(os.path.join('models', model_type))\n",
        "\n",
        "    #Loading checkpoint\n",
        "    saver.restore(session, ckpt)\n",
        "\n",
        "    print('Loading dataset...')\n",
        "    chunks = load_dataset(enc, dataset, 50000)\n",
        "    data_sampler = Sampler(chunks)\n",
        "    print('dataset has', data_sampler.total_size, 'tokens')\n",
        "    print('Training...')\n",
        "    counter = 1\n",
        "    counter_path = os.path.join(checkpoint_path, 'counter')\n",
        "    counter_base = counter\n",
        "\n",
        "    def sample_batch():\n",
        "      return [data_sampler.sample(1024) for _ in range(batch_size)]\n",
        "\n",
        "    avg_loss = (0.0, 0.0)\n",
        "    start_time = time.time()\n",
        "\n",
        "    if steps:\n",
        "        steps = int(steps)\n",
        "  #saving the trained model checkpoints \n",
        "    def save():\n",
        "      try:\n",
        "        os.makedirs(checkpoint_path)\n",
        "      except:\n",
        "        pass\n",
        "      print('Saving',os.path.join(checkpoint_path,'model-{}').format(counter-1))\n",
        "      saver.save(\n",
        "        session,\n",
        "        os.path.join(checkpoint_path, 'model'),\n",
        "        global_step=counter-1)\n",
        "      with open(counter_path, 'w') as fp:\n",
        "        fp.write(str(counter-1) + '\\n')\n",
        "\n",
        "    while True:\n",
        "        if steps > 0 and counter == (counter_base + steps):\n",
        "            save()\n",
        "            return\n",
        "\n",
        "        if accumulate_gradients > 1:\n",
        "            #execute the operation in the tensors\n",
        "            session.run(opt_reset)\n",
        "            for _ in range(accumulate_gradients):\n",
        "                session.run(\n",
        "                    opt_compute, feed_dict={context: sample_batch()})\n",
        "            (v_loss, v_summary) = session.run((opt_apply, summary_loss))\n",
        "        else:\n",
        "            (_, v_loss, v_summary) = session.run(\n",
        "                (opt_apply, loss, summary_loss),\n",
        "                feed_dict={context: sample_batch()})\n",
        "\n",
        "        summary_log.add_summary(v_summary, counter)\n",
        "\n",
        "        if counter % 20 == 0:\n",
        "            avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
        "                        avg_loss[1] * 0.99 + 1.0)\n",
        "\n",
        "            print(\n",
        "                '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
        "                .format(\n",
        "                    counter=counter,\n",
        "                    time=time.time() - start_time,\n",
        "                    loss=v_loss,\n",
        "                    avg=avg_loss[0] / avg_loss[1]))\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "\n",
        "#This method generates text from the loaded model.\n",
        "\n",
        "def generate(session,\n",
        "             run_name='run1',\n",
        "             model_type=None,\n",
        "             model_novel_output_file='model_output_novel_t7_le5000.txt',\n",
        "             restler_processed_output_file='RESTler_unique_output.txt',\n",
        "             prefix=None,\n",
        "             seed=None,\n",
        "             nsamples=1,\n",
        "             batch_size=1,\n",
        "             length=1023,\n",
        "             temperature=0.7,\n",
        "             top_k=0,\n",
        "             top_p=0.0,\n",
        "             include_prefix=True,\n",
        "             overwrite=False):\n",
        "\n",
        "\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    if prefix == '':\n",
        "        prefix = None\n",
        "\n",
        "    if model_type:\n",
        "        checkpoint_path = os.path.join('models', model_type)\n",
        "    else:\n",
        "        checkpoint_path = os.path.join('checkpoint', run_name)\n",
        "  #We should convert words(tokens) to numbers as model cannot understand texts.\n",
        "  #This vector of numbers capture some of meaning of word.\n",
        "    enc = encoder.get_encoder(checkpoint_path)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if prefix:\n",
        "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
        "        context_tokens = enc.encode(prefix)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "    output = sample.sample_sequence(\n",
        "        hparams=hparams,\n",
        "        length=min(length, 1023 - (len(context_tokens) if prefix else 0)),\n",
        "        start_token=enc.encoder['<|endoftext|>'] if not prefix else None,\n",
        "        context=context if prefix else None,\n",
        "        batch_size=batch_size,\n",
        "        temperature=temperature, top_k=top_k, top_p=top_p\n",
        "    )[:, 1:]\n",
        "    #If overwite is true it appends to the previous content\n",
        "    if overwrite:\n",
        "      f = open(\"model_output_raw.txt\", 'a')\n",
        "    else:\n",
        "      f = open(\"model_output_raw.txt\", 'w')\n",
        "    \n",
        "    generated = 0\n",
        "    while generated < nsamples:\n",
        "        if not prefix:\n",
        "            #execute the operation in the tensors\n",
        "            out = session.run(output)\n",
        "        else:\n",
        "            out = session.run(output, feed_dict={\n",
        "                    context: batch_size * [context_tokens]\n",
        "                })\n",
        "        for i in range(batch_size):\n",
        "            generated += 1\n",
        "            gen_text = enc.decode(out[i])\n",
        "            if prefix:\n",
        "                gen_text = enc.decode(context_tokens[:1]) + gen_text\n",
        "\n",
        "            gen_text = gen_text.lstrip('\\n')\n",
        "            #After each sample it adds END_OF_SAMPLE to the output file\n",
        "            f.write(\"{}\\n{}\".format(gen_text, \"END_OF_SAMPLE\\n\"))\n",
        "\n",
        "    f.close()\n",
        "\n",
        "\n",
        "#This file (model_output_raw.txt) contains all the output that our model generated.\n",
        "#We should process the file and extract just the novel and uniques ones. (model_output_unique.txt)\n",
        "\n",
        "    with open(\"model_output_raw.txt\", 'r') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    count=0;\n",
        "    for line in lines:\n",
        "      if \"END_OF_SAMPLE\" in line:\n",
        "        lines[count]=\"\"\n",
        "        lines[count-1]=\"\"\n",
        "      count=count+1\n",
        "        \n",
        "\n",
        "    model_output_unique = sorted(set(lines), key=lines.index)\n",
        "\n",
        "    if model_output_unique[-1][-1:] ==\"\\n\":\n",
        "      model_output_unique[-1]=model_output_unique[-1][:-1]\n",
        "      \n",
        "    with open('model_output_unique.txt', 'w') as rmdup:\n",
        "      rmdup.writelines(model_output_unique)\n",
        "\n",
        "    with open('model_output_unique.txt', 'r') as rmdup:\n",
        "      model_output_unique=rmdup.readlines()\n",
        "\n",
        "    #It is needed to compare the unique output of Restler and the model.\n",
        "    #We remove all the sapce from these files so that the space does not mislead us for the comparison process\n",
        "\n",
        "    # remove spaces\n",
        "    model_compressed_requests = [line.replace(' ', '') for line in model_output_unique]\n",
        "\n",
        "    with open(restler_processed_output_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "    # remove spaces\n",
        "    dataset_requests = [line.replace(' ', '') for line in lines]\n",
        "\n",
        "\n",
        "    count=0\n",
        "    new_request_lines=[]\n",
        "    for model_request in model_compressed_requests:\n",
        "      duplicate= False\n",
        "      for dataset_request in dataset_requests:\n",
        "        if model_request == dataset_request:\n",
        "          duplicate=True\n",
        "      if duplicate == False:\n",
        "        new_request_lines.append(count)\n",
        "      count=count+1\n",
        "\n",
        "    new_requests_file = open(model_novel_output_file, \"w\")\n",
        "    for line in new_request_lines:\n",
        "      new_requests_file.write(model_output_unique[line])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qUEQvLdr1_H1"
      },
      "outputs": [],
      "source": [
        "get_model(model_type=\"124M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBCzMX8V2AfA",
        "outputId": "1f7b78f4-5e14-48ce-9fdc-1e1db91d6869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'request_type', 'request_uri', 'request_body', 'response_code', 'response_time_microseconds']\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  8.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 26690 tokens\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20 | 99.71] loss=0.36 avg=0.36\n",
            "[40 | 192.62] loss=0.27 avg=0.31\n",
            "[60 | 285.45] loss=0.25 avg=0.29\n",
            "[80 | 378.24] loss=0.21 avg=0.27\n",
            "[100 | 471.01] loss=0.20 avg=0.26\n",
            "Saving checkpoint/run1/model-100\n"
          ]
        }
      ],
      "source": [
        "session = tensorflow_session()\n",
        "#with fine tuning we can train the model on our specific dataset\n",
        "fine_tuning(session,\n",
        "              restler_raw_file='test_cases_produced.csv',\n",
        "              model_type='124M',\n",
        "              steps=100,\n",
        "              run_name='run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "er3VC7If2C1c"
      },
      "outputs": [],
      "source": [
        "#This will predict the next strings after the prefix\n",
        "#The lenght specify the size of each sample\n",
        "#Higher temperature results in more random completions. temperature=0 the model will become deterministic.\n",
        "#If overwrite is True, it concatenates the generated requests to the previous one.\n",
        "generate(session,\n",
        "              length=5000,\n",
        "              temperature=0.7,\n",
        "              prefix=\"HTTP GET\",\n",
        "              nsamples=20,\n",
        "              batch_size=5,\n",
        "              overwrite=True\n",
        "              )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT_Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
